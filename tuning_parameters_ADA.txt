{
# --- Feature selection (SelectKBest) ---
# used only to modify selected SKB behavior
#'feature_selection__k': [25, 50], # number of top features to select based on univariate scores
#'feature_selection__score_func': [mutual_info_classif], # scoring function estimating mutual information for classification tasks

# --- Feature selection (SelectFromModel with L1 Logistic Regression) ---
# used only to modify laSFM behavior
#'feature_selection__threshold': [-float('inf')], # keep all features with non-zero weights (no threshold cutoff)
#'feature_selection__max_features': [25, 50], # number of features retained after selection
#'feature_selection__estimator__C': [0.1, 1.0], # inverse regularization strength controlling sparsity
#'feature_selection__estimator__max_iter': [300], # maximum iterations for convergence

# --- Feature selection (SelectFromModel with ElasticNet Logistic Regression) ---
# used only to modify enSFM behavior
#'feature_selection__threshold': [-float('inf')], # keep all non-zero weighted features (no threshold cutoff)
#'feature_selection__max_features': [25, 50], # number of retained features after selection
#'feature_selection__estimator__l1_ratio': [0.5], # mix of L1/L2 regularization
#'feature_selection__estimator__C': [1.0], # inverse regularization strength

# --- Feature selection (SelectFromModel with RandomForestClassifier) ---
# used only to modify rfSFM behavior
#'feature_selection__threshold': [-float('inf')], # keep all non-zero importance features
#'feature_selection__max_features': [25, 50], # limit the number of features kept based on feature importance
#'feature_selection__estimator__n_estimators': [200], # number of trees
#'feature_selection__estimator__max_depth': [5, 10], # tree depth control

# --- Model tuning (AdaBoost) ---
# used only to modify ADA behavior
'model__n_estimators': [50, 100], # fewer boosting rounds to speed up training while keeping enough estimators for stable importance
'model__learning_rate': [0.05, 0.1], # slightly larger steps to converge faster while maintaining balanced weight updates
'model__algorithm': ['SAMME.R'], # efficient real-valued boosting for smoother and quicker convergence
'model__estimator': [DecisionTreeClassifier(max_depth=d, min_samples_leaf=5) for d in (1, 2)] # shallower weak learners (depth 1â€“2) for faster fitting and sufficiently informative feature importances
}
